{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Members:\n",
    "\n",
    "- Name, matriculation number\n",
    "- Name, matriculation number\n",
    "- Name, matriculation number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Auto-Encoder (AE), Out-of-Distribution (OOD) Detection and Open-Set Recognition (OSR)\n",
    "\n",
    "In this assignment, we show that it is possible to perform a (non-linear) dimensionality reduction by learning from unlabeled data using a convolutional auto-encoder network.\n",
    "The task is to reduce an image of the handwritten digits of MNIST into a deep feature representation, without making use of their labels, and reconstruct the sample from that representation.\n",
    "\n",
    "For this purpose, we implement a convolutional auto-encoder that is composed of two sub-networks.\n",
    "An *encoder* network composed of several strided convolution layers, which finally performs an embedding in a $K=10$-dimensional deep feature representation.\n",
    "The *decoder* uses this deep feature representation to reconstruct an image in the original size of $28\\times28$ pixels by applying a series of fractionally-strided transposed convolutional layers.\n",
    "\n",
    "After training such a network, we show that this has learned well the distribution of the known classes, which are represented by the MNIST test sample.\n",
    "On the other hand, we show that this network has limited representation capabilities for *near OOD* samples coming from the related EMNIST letters dataset.\n",
    "Also, we use *far OOD* samples coming from the Fashion-MNIST dataset to show that such samples are not represented in the embedding space of our auto-encoder.\n",
    "\n",
    "For both of these types of OOD samples, we show that our auto-encoder network can be used to detect out-of-distribution data in the test set, and perform open-set recognition.\n",
    "Particularly, we make use of the reconstruction error of our auto-encoder to distinguish known and OOD samples.\n",
    "Additionally, we compute centroids of embedding vectors for all known classes, and use distances to such classes for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Theoretical Questions\n",
    "\n",
    "In this section we first investigate core questions regarding auto-encoder networks, for example, how to achieve a vector representation for the deep features. Afterward, we discuss some characteristics of approaches to open-set recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: PCA vs. Two-Layer Autoencoder\n",
    "\n",
    "Principal Component Analysis (PCA) and two-layer AutoEncoders (AE) are two different ways for projecting a vector input vector $\\vec x \\in \\mathbb R^D$ into a vector representation $\\vec \\varphi \\in \\mathbb R^K$ with $K \\ll D$.\n",
    "In both cases, a projection matrix $\\mathbf W$ is learned to perform this task.\n",
    "For PCA, this projection is applied as $\\vec\\varphi = \\mathbf W(\\vec x - \\tilde x)$ where $\\tilde x$ is the mean of the samples over the training set.\n",
    "A two-layer autoencoder is composed of an encoder with one fully-connected layer $\\mathbf W^{(1)} \\in \\mathbb R^{K\\times(D+1)}$, and a decoder containing one fully-connected layer $\\mathbf W^{(2)} \\in \\mathbb R^{(K+1)\\times D}$, each including a bias neuron.\n",
    "\n",
    "**Question**: What is the difference between PCA and the two-layer autoencoder with respect to the embedding matrix $\\mathbf W$, and the learning procedure? Is there any theoretical difference how the bias is handled between the two methods? \n",
    "\n",
    "**Answers** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Options for One-Dimensional Feature Vectors in Latent Space\n",
    "\n",
    "In a convolutional autoencoder, the encoder typically employs a series of convolutional layers, which provides feature maps $\\mathcal A\\in\\mathbb R^{Q\\times K\\times M}$. \n",
    "The embedding vector, however, is required to be a vector with a desired number of elements.\n",
    "\n",
    "**Question**: What are the different approaches to obtaining a one-dimensional feature vector in the latent space of autoencoders?\n",
    "\n",
    "**Answers**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Two-Step Approach for OSR - Pros and Cons\n",
    "\n",
    "In open-set recognition, two-stage approaches first perform a binary classification of a sample to be *known* or *unknown*, and then classify all *known* samples as their respective class.\n",
    "\n",
    "**Question**: What are the advantages and disadvantages of employing such a two-step approach for Open-Set Recognition?\n",
    "\n",
    "**Answers**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4: Entropic Open-Set (EOS) Loss Bias\n",
    "\n",
    "The Entropic Open-Set (EOS) loss aka. the Softmax Adaptation requires having no bias in the final fully-connected layer that transforms the deep features $\\vec\\varphi$ to the logits: $\\vec z = W^{(L)} \\vec\\varphi$.\n",
    "\n",
    "**Question**: Why is it theoretically impossible to introduce a bias for the Entropic Open-Set (EOS) loss?\n",
    "\n",
    "**Answer**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.5: Selection of Negative Samples\n",
    "\n",
    "EOS loss requires *negative* samples for training.\n",
    "\n",
    "**Question**: Given a specific set of known classes, how should negative samples be selected? Can random noise be used as negatives?\n",
    "\n",
    "**Answer**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.6: Training with Negative Samples for Different OOD Detection Approaches\n",
    "\n",
    "As mentioned above, we will make use of two methods for performing Out-of-Distribution (OOD) detection.\n",
    "One is based on thresholding reconstruction error values, and one is based on distances in deep feature space.\n",
    "More details on these two methods can be found at later sections in this notebook.\n",
    "\n",
    "Theoretically, we could add the training samples of our unknown datasets (EMNIST letters, FashionMNIST) to our original training data (MNIST), and use all of these to train our autoencoder network.\n",
    "\n",
    "**Question**: What do you expect: Is it advisable to train with additional samples when using autoencoders for OOD detection? Is there a difference between the two methods for OOD, i.e., thresholding reconstruction error or thresholding distances to class centroids in the deep feature space?\n",
    "\n",
    "**Answer**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Coding\n",
    "\n",
    "**<font color='red' size='5'>This section has to be submitted by 11:59 p.m. on Wednesday, May 7th, to be graded.</font>**\n",
    "\n",
    "The coding part is threefold.\n",
    "First, we define everything that is required to train and evaluate our autoencoder network.\n",
    "This includes the data sets used for training, validation and testing, as well as the datasets that serve as unknown samples for the OOD detection.\n",
    "\n",
    "Afterward we define our convolutional autoencoder by defining an `Encoder` module and a `Decoder` module, which we combine to an auto-encoder.\n",
    "We instantiate the auto-encoder and all other variables needed for training, and implement the training and validation loop.\n",
    "Finally, we train our auto-encoder on our training dataset.\n",
    "\n",
    "Finally, we use two different techniques for out-of-distribution detection using our train autoencoder network.\n",
    "First, we use the reconstruction error on our known and unknown datasets to distinguish between known and unknown samples.\n",
    "Then, we use distances in deep feature space to perform the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    print('Using CPU. Warning: This will be slow!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Datasets and Data Loaders\n",
    "\n",
    "We will make use of `torchvision.datasets.EMNIST` dataset and `torchvision.datasets.FashionMNIST` dataset.\n",
    "EMNIST has two splits, `mnist` and `letters`, with 10 labels of digit images and 26 labels of letter images respectively. \n",
    "FashionMNIST has 10 labels of merchandise images.\n",
    "However, for the training of the Auto-Encoder (AE) we do not make use of the labels of the dataset, but we only utilize the images.\n",
    "For testing purposes, we assign label `-1` to all unknown samples -- note that this is different from the theoretical part in the lecture, where we assign label `0` as unknown.\n",
    "\n",
    "These datasets can be split into train sets and test sets by the default implementation.\n",
    "We will use solely the MNIST split for training and validation of the AE network.\n",
    "To evaluate this network for out-of-distribution detection, we will use the test sets from EMNIST letters and FashionMNIST respectively.\n",
    "\n",
    "1. Create the `train_dataset` for the EMNIST `mnist` split.\n",
    "2. Create test datasets for all datasets. For the test data loaders set `shuffle=True` for the sampling of images to be simpler, even though normally we would not shuffle the test set.\n",
    "3. Create dataloaders for all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "\n",
    "# MNIST dataset\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "# Create dataset with open-set samples (EMNIST letters)\n",
    "def rotate_image(image):\n",
    "    # Rotate the image by 90 degrees\n",
    "    return image.transpose(2, 1)\n",
    "emnist_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    rotate_image,\n",
    "])\n",
    "\n",
    "# training set of mnist digits\n",
    "train_dataset = ...\n",
    "train_loader = ...\n",
    "\n",
    "# test dataset of mnist digits\n",
    "test_dataset = ...\n",
    "test_loader = ...\n",
    "\n",
    "# Create dataset and dataloader for EMNIST letters\n",
    "emnist_letters_dataset = ...\n",
    "# Assign -1 label to all EMNIST letter samples\n",
    "emnist_letters_dataset.targets = ...\n",
    "\n",
    "emnist_letters_loader = ...\n",
    "\n",
    "\n",
    "# Create dataset and dataloader for FashionMNIST\n",
    "fashion_mnist_dataset = ...\n",
    "# Assign -1 label to all FashionMNIST samples\n",
    "fashion_mnist_dataset.targets = ...\n",
    "fashion_mnist_loader = ...\n",
    "\n",
    "# print the number of samples\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of EMNIST letters samples: {len(emnist_letters_dataset)}\")\n",
    "print(f\"Number of FashionMNIST samples: {len(fashion_mnist_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at some images to get an idea of the data we are working with.\n",
    "Notably, the letters are much more similar to the training data than the FashionMNIST data, therefore we call the letters near-OOD while the FashionMNIST data is far-OOD.\n",
    "\n",
    "Note: We only look at the test sets now, to make sure that they are implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some images from each dataset\n",
    "def show_images(data_loader, title, nr_images = 10):\n",
    "    # sample images from the dataset\n",
    "    images, labels = next(iter(data_loader))\n",
    "    fig, axs = plt.subplots(1, nr_images, figsize=(nr_images, 2))\n",
    "    fig.suptitle(title)\n",
    "    for i in range(nr_images):\n",
    "        axs[i].imshow(images[i][0], cmap='gray')\n",
    "        axs[i].set_title(labels[i].item())\n",
    "        axs[i].axis('off')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "datasets = {\n",
    "    'train': train_loader,\n",
    "    'test': test_loader,\n",
    "    'emnist_letters': emnist_letters_loader,\n",
    "    'fashion_mnist': fashion_mnist_loader\n",
    "}\n",
    "for name, loader in datasets.items():\n",
    "    show_images(loader, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-Encoder Network\n",
    "\n",
    "The auto-encoder network is composed of two parts: the encoder that transforms the input image to a deep feature representation; and the decoder that produces an image from such a deep feature.\n",
    "\n",
    "For the encoder $\\mathcal E$, we will use a convolutional network and perform down-sampling via striding.\n",
    "After each convolution, we apply the ReLU activation.\n",
    "The output of the encoder is a $K=10$ dimensional deep feature representation.\n",
    "The complete encoder network topology can be found below in Topology 1(a).\n",
    "\n",
    "The decoder $\\mathcal D$ performs the inverse operations of the encoder.\n",
    "A fully-connected layer is used to increase the number of samples to the same size as the output of the flattening of the encoder.\n",
    "Then, the flattening needs to be undone by reshaping the vector into the correct dimensionality, followed by a ReLU activation.\n",
    "A fractionally-strided convolutional layer increases the intermediate representation by a factor of 2.\n",
    "Note that the fractionally-strided convolution is implemented in `torch.nn.ConvTranspose2d`, and the `stride` parameter should have the same value as for the encoder.\n",
    "Additionally, the `torch.nn.ConvTranspose2d` has a parameter `output_padding` which needs to be adapted to reach the correct output shape (see Test 2).\n",
    "After this layer, we perform another ReLU activation and another fractionally-strided convolution to arrive at the original input dimension.\n",
    "The complete decoder network topology can be found below in Topology 1(b).\n",
    "\n",
    "Finally, we combine the two sub-networks into one auto-encoder network.\n",
    "While there exist several possibilities for doing this, we will implement a third `torch.nn.Module` that contains an instance of the encoder and an instance of the decoder.\n",
    "\n",
    "Topology 1: Network configurations of the (a) encoder and (b) decoder networks\n",
    "\n",
    "(a) Encoder Network\n",
    "\n",
    "*   2D convolutional layer with $Q_1$ channels, kernel size $5\\times5$, **stride 2** and padding 2\n",
    "*   activation function ReLU\n",
    "*   2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, **stride 2** and padding 2\n",
    "*   flatten layer to convert the convolution output into a vector\n",
    "*   activation function ReLU\n",
    "*   fully-connected layer with the correct number of inputs and $K$ outputs\n",
    "\n",
    "(b) Encoder Network\n",
    "\n",
    "*   fully-connected layer with $K$ inputs and the correct number of outputs\n",
    "*   activation function ReLU\n",
    "*   reshaping to convert the vector into a convolution input\n",
    "*   2D **fractionally-strided convolutional** layer with $Q_2$ channels, kernel size $5\\times5$, stride 2 and padding 2\n",
    "*   activation function ReLU\n",
    "*   2D **fractionally-strided convolutional** layer with $Q_1$ channels, kernel size $5\\times5$, stride 2 and padding 2\n",
    "\n",
    "### Task 2.2: Encoder Network\n",
    "Implement the encoder network for given parameters $Q_1$, $Q_2$, and $K$ as given in Topology 1(a).\n",
    "Implement a network class that derives from `torch.nn.Module` and implement the `__init__` and the `forward` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "  def __init__(self, Q1, Q2, K):\n",
    "    # call base class constrcutor\n",
    "    super(Encoder,self).__init__()\n",
    "    # convolutional define layers\n",
    "    self.conv1 = ...\n",
    "    self.conv2 = ...\n",
    "    # activation functions will be re-used for the different stages\n",
    "    self.act = ...\n",
    "    # define fully-connected layers\n",
    "    self.flatten = ...\n",
    "    self.fc = ...\n",
    "\n",
    "  def forward(self, x):\n",
    "    # get the deep feature representation\n",
    "    deep_feature = ...\n",
    "    return deep_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Decoder Network\n",
    "\n",
    "Implement the decoder network for given parameters $Q_1$, $Q_2$, and $K$ as given in Topology 1(b).\n",
    "Implement a network class that derives from `torch.nn.Module` and implement the `__init__` and the `forward` methods.\n",
    "The output of the decoder network is supposed to have values in the range $[0,1]$, similar to the input values.\n",
    "We need to make sure that only these values can be achieved.\n",
    "Think of possible ways of doing that, and apply the way that seems most reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "  def __init__(self, Q1, Q2, K):\n",
    "    # call base class constrcutor\n",
    "    super(Decoder,self).__init__()\n",
    "    # fully-connected layer\n",
    "    self.fc = ...\n",
    "    # convolutional layers\n",
    "    self.deconv1 = ...\n",
    "    self.deconv2 = ...\n",
    "    # activation function\n",
    "    self.act = ...\n",
    "    self.act1 = ...\n",
    "\n",
    "  def forward(self, x):\n",
    "    # reconstruct the output image\n",
    "    output = ...\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Joint Auto-Encoder Network\n",
    "\n",
    "Implement the auto-encoder network by combining the encoder and the decoder.\n",
    "In the `__init__` function, instantiate an encoder from Task 2 and a decoder from Task 3.\n",
    "In `forward`, pass the input through the encoder and the decoder: $\\mathbf Y = \\mathcal D(\\mathcal E(\\mathbf X))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "  def __init__(self, Q1, Q2, K):\n",
    "    super(AutoEncoder,self).__init__()\n",
    "    self.encoder = ...\n",
    "    self.decoder = ...\n",
    "\n",
    "  def forward(self, x):\n",
    "    # encode input\n",
    "    deep_feature = ...\n",
    "    # decode to output\n",
    "    reconstructed = ...\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Output Sizes\n",
    "\n",
    "The code below instantiates the auto-encoder network with $Q_1 = Q_2 = 32$ and $K=10$.\n",
    "Then the given input $\\mathbf X$ is provided to the (untrained) auto-encoder network.\n",
    "Use these codes to verify that the deep feature extracted by the encoder and the output from the decoder part both have the desired size. Also, verify that the output values are between 0 and 1.\n",
    "\n",
    "If the tests cannot be passed, please check the implementations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "model_ = AutoEncoder(32, 32, 10).to(device)\n",
    "\n",
    "# create or select a sample\n",
    "x = torch.randn((1,1,28,28), device=device)\n",
    "\n",
    "# use encoder to encode image and check its size\n",
    "deep_features_ = model_.encoder(x)\n",
    "assert deep_features_.shape[1] == 10\n",
    "\n",
    "# use decoder to generate an image and check its size and value range\n",
    "output_ = model_.decoder(deep_features_)\n",
    "assert output_.shape[2:] == (28,28)\n",
    "assert torch.all(output_ >= 0) and torch.all(output_ <= 1)\n",
    "\n",
    "# make sue that the auto-encoder is implemented correctly\n",
    "output__ = model_(x)\n",
    "assert output__.shape[2:] == (28,28)\n",
    "assert torch.allclose(output_, output__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "We will implement a training procedure for an auto-encoder network.\n",
    "\n",
    "To train the network, we will use the $L_2$ distance between the output and the input of the network as a loss function, which is implemented in `torch.nn.MSELoss`:\n",
    "\n",
    "  $$\\mathcal J^{L_2} (\\mathbf X, \\mathbf Y) = \\|\\mathbf X - \\mathbf Y\\|^2$$\n",
    "\n",
    "For optimization, we will make use of the `Adam` optimizer with a learning rate of $\\eta=0.001$.\n",
    "We will run the training for 10 epochs and compute training and validation set loss after each epoch.\n",
    "\n",
    "For evaluation, we will check whether some of the validation set samples are correctly reconstructed from the auto-encoder network by visualizing them.\n",
    "\n",
    "In our test sets, there are 10000 in-distribution samples and 20800+10000 OOD samples.\n",
    "In the case of evaluating the success rate of anomaly detection, accuracy is not the best metric because it gives equal weights to two classes with unequal numbers of samples, while the actual distribution of the two classes is unbalanced.\n",
    "A system can achieve high accuracy by simply predicting the majority class for every instance, while completely ignoring the minority class.\n",
    "\n",
    "In such cases, we choose to use the true positive rate (TPR) and true negative rate (TNR), which measure the proportion of actual positive/negative cases that are correctly identified by the system.\n",
    "\n",
    "$$\\text{TPR} = \\frac{\\text{True Positives}}{\\text{True Positives+False Negatives}}$$\n",
    "\n",
    "$$\\text{TNR} = \\frac{\\text{True Negatives}}{\\text{True Negatives+False Positives}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5 Training Loop\n",
    "\n",
    "Instantiate the auto-encoder network with $Q_1 = Q_2 = 32$ and $K=10$.\n",
    "\n",
    "To train the auto-encoder network, we will use the $L_2$ distance between the output and the input of the network as a loss function.\n",
    "This loss function is implemented in `torch.nn.MSELoss` and make sure to use `reduction='none'` as this will allow us to compute the loss for each sample separately, which will become important later.\n",
    "This will keep the output size identical to the input size.\n",
    "However, because we still need to compute the average loss for the backpropagation and to report the training and validation performance, we need to manually compute these averages.\n",
    "For `sample_J`, compute the mean loss for each sample over the data dimensions.\n",
    "For `J` compute the average of `sample_J` over the batch size.\n",
    "\n",
    "Since training an auto-encoder is tricky, we will make use of the Adam optimizer.\n",
    "Choose a learning rate of $\\eta=0.001$. Implement the training loop for 10 epochs.\n",
    "\n",
    "Note: If the training and validation loss does not decrease during training, try to reduce the learning rate (to $\\eta=0.0005$ or even lower) and re-start the training (remember to re-initialize the network, too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = ...\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = ...\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_function = ...\n",
    "optimizer = ...\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for X, _ in tqdm(train_loader):\n",
    "\n",
    "        # Forward pass\n",
    "        Y = ...\n",
    "        # Compute loss per sample. sample_J is a tensor of shape (batch_size,)\n",
    "        sample_J = ...\n",
    "        # Compute average loss over batch\n",
    "        J = ...\n",
    "\n",
    "        # Backward and optimize\n",
    "        ...\n",
    "\n",
    "        # Add sample-wise loss to list\n",
    "        ...\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    return losses\n",
    "\n",
    "# test (i.e. Inference)\n",
    "def test(data_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for X, _ in tqdm(data_loader):\n",
    "\n",
    "            # Forward pass\n",
    "            Y = ...\n",
    "\n",
    "            # Compute loss per sample. sample_J is a tensor of shape (batch_size,)\n",
    "            sample_J = ...\n",
    "\n",
    "            # Add sample-wise loss to list\n",
    "            ...\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Loss Computation\n",
    "\n",
    "We test that the shape of the loss values is correct to assure that the reduction is only applied to each image and the loss for each sample is reported separately.\n",
    "\n",
    "With the default parameters from the tasks above, you can expect a loss of roughly 0.015-0.020 for the training and in-distribution test set after 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        # Generate random samples and targets\n",
    "        self.data = torch.rand(*self.shape)  # Random values between 0 and 1\n",
    "        self.targets = torch.randint(0, 10, (self.shape[0],))  # Random integer labels from 0 to 9\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.shape[0]:\n",
    "            raise IndexError(f\"Index {idx} out of range for dataset of size {self.shape[0]}\")\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "data_size_ = 5\n",
    "dataset_ = RandomImageDataset(shape=(data_size_, 1, 28, 28))\n",
    "dataloader_ = DataLoader(dataset_, batch_size=data_size_, shuffle=False)\n",
    "losses_ = test(dataloader_)\n",
    "\n",
    "assert losses_.shape == (data_size_,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the model on our training dataset, and perform the validation on our validation dataset.\n",
    "Since this is the same as in previous exercises, we have implemented this boring part for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train().mean()\n",
    "    validation_loss = test(test_loader).mean()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(validation_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Train Loss: {train_loss:.6f}, Average Validation Loss: {validation_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we plot the training and validation set losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "def plot_losses():\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Distribution (OOD) Detection\n",
    "\n",
    "In this task, perform out-of-distribution (OOD) detection via the reconstruction error from the trained network.\n",
    "We will use the test set of EMNIST letters and FashionMNIST as OOD samples.\n",
    "We will first reconstruct the images of the three test sets.\n",
    "Then we will perform OOD detection by computing the sample-wise loss thresholding the reconstruction error.\n",
    "Samples with an error below a certain threshold are considered in-distribution, while samples with an error above the threshold are considered OOD.\n",
    "\n",
    "Forward the re-assigned loss as predictions into the function defined above to compute TPR and TNR. Compute the regular accuracy as well to make a comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6: Reconstruction Result\n",
    "\n",
    "This task is to visualize the reconstructed images from their originals.\n",
    "For this purpose, load the first batch of each of the test sets. \n",
    "Forward the images through the trained auto-encoder network to extract their reconstructions.\n",
    "Make a single plot with 2 rows and 10 columns. \n",
    "In the 1st row, plot the original samples and in the 2nd row plot the corresponding reconstructed samples. \n",
    "\n",
    "Then, for each dataset, compute the average reconstruction error of the entire dataset and print it.\n",
    "Hint: you can reuse the `test()` function from task 2.5.\n",
    "\n",
    "You should be able to see that the reconstruction error is much larger for the OOD samples (especially for far-OOD samples) than for the in-distribution samples.\n",
    "This is a good sign that the auto-encoder network is able to detect OOD samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of some reconstructed images\n",
    "def visualize_reconstruction(data_loader, data_set_name, num_images=10):\n",
    "    assert num_images <= batch_size, \"num_images should be less than or equal to batch size\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of test images\n",
    "        X = ...\n",
    "\n",
    "        # Generate reconstructions\n",
    "        Y = ...\n",
    "\n",
    "        # Move tensors to CPU\n",
    "        X = ...\n",
    "        Y = ...\n",
    "\n",
    "        # Plot original vs reconstructed images\n",
    "        fig, axs = plt.subplots(2, num_images, figsize=(num_images, 3))\n",
    "        fig.suptitle(f\"Reconstruction on {data_set_name}\")\n",
    "        axs = axs.flatten()\n",
    "\n",
    "        for i in range(num_images):\n",
    "            # Original images\n",
    "            ax = axs[i]\n",
    "            ax.imshow(X[i][0], cmap='gray')\n",
    "            ax.set_title(\"Original\", fontsize=8)\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Reconstructed images\n",
    "            ax = axs[i + num_images]\n",
    "            ax.imshow(Y[i][0], cmap='gray')\n",
    "            ax.set_title(\"Reconstructed\", fontsize=8)\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "visualize_reconstruction(..., \"Test Set\")\n",
    "reconstruction_error_test = ...\n",
    "print(f'Average Test Reconstruction Error: {reconstruction_error_test:.6f}')\n",
    "\n",
    "# Visualize some open-set samples for EMNIST letters and FashionMNIST\n",
    "\n",
    "# EMNIST letters\n",
    "visualize_reconstruction(..., \"EMNIST Letters\")\n",
    "# Evaluate reconstruction error on open-set samples\n",
    "reconstruction_error_emnist = ...\n",
    "print(f'Average Open-set Test Reconstruction Error on EMNIST Letters: {reconstruction_error_emnist:.6f}')\n",
    "\n",
    "# FashionMNIST\n",
    "visualize_reconstruction(..., \"FashionMNIST\")\n",
    "# Evaluate reconstruction error on open-set samples\n",
    "reconstruction_error_fashion = ...\n",
    "print(f'Average Open-set Test Reconstruction Error on Fashion MNIST: {reconstruction_error_fashion:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.7: Visualizing the OOD Signal\n",
    "\n",
    "We now exploit the inability of the AE network to reconstruct OOD samples, by using the reconstruction error as a signal for OOD detection and applying a threshold to the reconstruction error.\n",
    "Determine a reasonable `ood_threshold` from the plot of this task (you need to run the plotting with value `0` first, and then you can decide for a good threshold) that can be used classify the samples as in-distribution or OOD.\n",
    "Compute and plot the distribution of test errors on all three datasets as a histogram along with the threshold, in order to visualize the OOD detection performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction_error_distributions(test_loader, near_ood_loader, far_ood_loader, threshold):\n",
    "    test_errors = ...\n",
    "    near_ood_errors = ...\n",
    "    far_ood_errors = ...\n",
    "\n",
    "    all_errors = torch.cat([torch.tensor(test_errors), torch.tensor(near_ood_errors), torch.tensor(far_ood_errors)])\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bins = torch.linspace(0, all_errors.max(), 50)\n",
    "    plt.hist(test_errors, bins=bins, alpha=0.5, color='green', label='MNIST')\n",
    "    plt.hist(near_ood_errors, bins=bins, alpha=0.5, color='blue', label='Near-OOD Samples')\n",
    "    plt.hist(far_ood_errors, bins=bins, alpha=0.5, color='red', label='Far-OOD Samples')\n",
    "    plt.axvline(threshold, color='black', linestyle='--', label='Threshold')\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Test Reconstruction Error Distributions')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# define a reasonable threshold for distinguishing ID and OOD samples, using this plot\n",
    "ood_threshold = ...\n",
    "plot_reconstruction_error_distributions(test_loader, emnist_letters_loader, fashion_mnist_loader, ood_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.8: True Positive/Negative Rate Calculation\n",
    "\n",
    "Define a function that takes the predictions and truth values, as lists, and returns TPR and TNR.\n",
    "\n",
    "You can use `sklearn.metrics.confusion_matrix` (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to compute true positives, true negatives, false positives, and false negatives, or compute them by their definitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tpr_tnr(predictions, truth):\n",
    "    # Compute the confusion matrix, directly from the torch tensors\n",
    "    tn, fp, fn, tp = ...\n",
    "    # Compute TPR and TNR\n",
    "    tpr = ...\n",
    "    tnr = ...\n",
    "\n",
    "    return tpr, tnr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: TPR & TNR Calculation Check\n",
    "\n",
    "With the given truth values and predictions, call the function defined in Task 2.8 and check the returned TPR and TNR are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = torch.tensor([1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1])\n",
    "predictions = torch.tensor([1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1])\n",
    "\n",
    "# Compute FMR and FNMR\n",
    "tpr, tnr = compute_tpr_tnr(predictions, truth)\n",
    "\n",
    "assert(abs(tpr - 14/15) < 1e-8)\n",
    "assert(abs(tnr - 0.8) < 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.9: Evaluating OOD Detection\n",
    "\n",
    "Now we will perform OOD detection by computing the sample-wise loss\n",
    "In order to evaluate the performance, we need to convert the labels into binary labels and assign binary predictions.\n",
    "Assign 1 to samples with a reconstruction error smaller than your `ood_threshold`, and -1 otherwise.\n",
    "We adjust the labels accordingly.\n",
    "Then, compute the TPR and TNR for the three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "labels = []\n",
    "for dataloader in [test_loader, emnist_letters_loader, fashion_mnist_loader]:\n",
    "    losses = ...\n",
    "\n",
    "    # Compute binary predictions and labels: 1 for in-distribution, -1 for out-of-distribution\n",
    "    binary_prediction = ...\n",
    "    binary_labels = torch.where(dataloader.dataset.targets == -1, -1, 1)\n",
    "\n",
    "    predictions.append(binary_prediction)\n",
    "    labels.append(binary_labels)\n",
    "predictions = torch.cat(predictions)\n",
    "truth = torch.cat(labels)\n",
    "\n",
    "# Compute TPR and TNR\n",
    "tpr, tnr = compute_tpr_tnr(predictions, truth)\n",
    "print(f'True Positive Rate (TPR): {tpr:.6f}')\n",
    "print(f'True Negative Rate (TNR): {tnr:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-Set Recognition\n",
    "\n",
    "So far we have ignored the class labels of the training data and only made the binary distinction between in and out-of-distribution samples.\n",
    "In this task, we will use the class labels of the training data to perform open-set recognition, which also considers the class labels of the in-distribution samples.\n",
    "We will show that we can solve the in-distribution classification task with the encoder network that was trained in an unsupervised fashion.\n",
    "\n",
    "### Task 2.10: OSR via Class Distance\n",
    "\n",
    "We will perform the in-distribution classification task by computing class centroids, i.e. the average deep feature representations of the training data for each class.\n",
    "Then we will compute the Euclidean distance between the deep feature representation of the test sample and the class centroids.\n",
    "The class with the smallest distance is assigned to the test sample.\n",
    "Instead of using the reconstruction error as a signal of OODness, we will use the distance between the deep feature representation of the sample and the class centroids as signal of OODness and apply a threshold to the minimal distance of any test sample to all class centroids.\n",
    "\n",
    "- Compute the class centroids for the training set. \n",
    "    - Store them in a torch tensor of shape ($O$, $K$), where $K$ is the deep feature dimension and $O$ is the number of known classes.\n",
    "    - It is advised to compute a dictionary first with the class labels as keys and a list of all the deep feature representations as values, then compute the mean of each list and store it in the tensor.\n",
    "- Compute the distances of each sample to each class center. Use the `torch.cdist` function to compute the Euclidean distances for an entire batch at once.\n",
    "- Predict the classes for the test samples as follows:\n",
    "    - If for a sample the minimum distance to any class centroid is larger than the `osr_threshold`, assign it the label `-1` (unknown).\n",
    "    - Otherwise, assign it the label of the class centroid with the smallest distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSRClassifier():\n",
    "    def __init__(self, autoencoder):\n",
    "        self.encoder = autoencoder.encoder\n",
    "        self.decoder = autoencoder.decoder\n",
    "\n",
    "    def compute_centroids(self, data_loader):\n",
    "        self.encoder.eval()\n",
    "        class_features = {}\n",
    "        with torch.no_grad():\n",
    "            for X, Y in data_loader:\n",
    "                deep_features = ...\n",
    "                for i, label in enumerate(Y):\n",
    "                    # append features for this label\n",
    "                    class_features[label.item()].append(...)\n",
    "\n",
    "        # Initialize centroids in reasonable data structure\n",
    "        self.centroids = ...\n",
    "        # Compute the mean of the deep features for each class\n",
    "        for label in class_features:\n",
    "            self.centroids[label] = ...\n",
    "\n",
    "    def compute_distances(self, data_loader):\n",
    "        self.encoder.eval()\n",
    "        distances = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for X, Y in data_loader:\n",
    "                # extract deep features from the autoencoder\n",
    "                deep_features = ...\n",
    "\n",
    "                # Compute distances to all centroids for each sample\n",
    "                dist = ...\n",
    "                distances.append(dist)\n",
    "                labels.append(Y)\n",
    "\n",
    "        distances = torch.cat(distances)\n",
    "        labels = torch.cat(labels)\n",
    "\n",
    "        return distances, labels\n",
    "\n",
    "    def predict(self, data_loader, threshold):\n",
    "        self.encoder.eval()\n",
    "        # compute distances to the stored centroids\n",
    "        distances, Y = ...\n",
    "\n",
    "        # Make predictions based on the threshold\n",
    "        predictions = ...\n",
    "\n",
    "        return predictions, Y\n",
    "\n",
    "    def plot_centroid_reconstruction(self, num_images=10):\n",
    "        self.encoder.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            reconstruction = self.decoder(self.centroids.clone().to(device)).cpu()\n",
    "\n",
    "            # Plot the centroids\n",
    "            fig, axs = plt.subplots(1, num_images, figsize=(num_images, 2))\n",
    "            fig.suptitle(\"Centroid Reconstructions\")\n",
    "            for i in range(num_images):\n",
    "                axs[i].imshow(reconstruction[i][0], cmap='gray')\n",
    "                axs[i].set_title(f\"Class {i}\")\n",
    "                axs[i].axis('off')\n",
    "            plt.show()\n",
    "\n",
    "# Create OSR classifier for the autoencoder\n",
    "osr_classifier = OSRClassifier(model)\n",
    "# compute centroids for the training set\n",
    "osr_classifier.compute_centroids(train_loader)\n",
    "# Plot reconstructed centroids\n",
    "osr_classifier.plot_centroid_reconstruction(num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.11: Visualization of Distances to Class Centroids\n",
    "Plot the distances of the test samples to the class centroids as a histogram.\n",
    "Make sure to plot the distances for the three datasets in a single plot.\n",
    "Assign `ood_threshold=0` in the first round to plot the distributions.\n",
    "Then, set it to a reasonable value and call this plot again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distances of the validation set\n",
    "def plot_distances(osr_classifier, test_loader, near_ood_loader, far_ood_loader, threshold):\n",
    "    test_distances = ...\n",
    "    near_ood_distances = ...\n",
    "    far_ood_distances = ...\n",
    "\n",
    "    # compute min distances\n",
    "    test_distances = ...\n",
    "    near_ood_distances = ...\n",
    "    far_ood_distances = ...\n",
    "\n",
    "    # concatenate distances\n",
    "    all_distances = torch.cat([test_distances, near_ood_distances, far_ood_distances])\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bins = torch.linspace(0, all_distances.max(), 50)\n",
    "    plt.hist(test_distances, bins=bins, alpha=0.5, color='green', label='MNIST')\n",
    "    plt.hist(near_ood_distances, bins=bins, alpha=0.5, color='blue', label='Near-OOD Samples')\n",
    "    plt.hist(far_ood_distances, bins=bins, alpha=0.5, color='red', label='Far-OOD Samples')\n",
    "    plt.axvline(threshold, color='black', linestyle='--', label='Threshold')\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distance to Class Centroids')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "osr_threshold = ...\n",
    "osr_classifier.compute_distances(test_loader)\n",
    "plot_distances(osr_classifier, test_loader, emnist_letters_loader, fashion_mnist_loader, osr_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.12: OSR Evaluation\n",
    "\n",
    "In this task, we will evaluate the performance of the open-set recognition system on the test sets.\n",
    "We will use the same evaluation metrics as in the OOD detection task, i.e. TPR and TNR.\n",
    "In addition, we compute the closed-set accuracy on the known classes and plot the corresponding confusion matrix.\n",
    "Make sure to only compute the accuracy and confusion matrix on the known classes, i.e. the samples with labels 0-9.\n",
    "\n",
    "Note: There exist evaluation metrics such as the OSCR (Open Set Classification Rate) that are more suitable for open-set recognition tasks, but we will use the TPR and TNR metrics for simplicity here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute TPR and TNR on all datasets\n",
    "predictions = []\n",
    "labels = []\n",
    "for dataloader in [test_loader, emnist_letters_loader, fashion_mnist_loader]:\n",
    "    pred, label = ...\n",
    "    predictions.append(pred)\n",
    "    labels.append(label)\n",
    "\n",
    "predictions = torch.cat(predictions).cpu()\n",
    "labels = torch.cat(labels).cpu()\n",
    "\n",
    "# Compute binary predictions and labels: 1 for in-distribution, -1 for out-of-distribution\n",
    "binary_labels = torch.where(labels == -1, -1, 1)\n",
    "binary_predictions = torch.where(predictions == -1, -1, 1)\n",
    "\n",
    "# Compute TPR and TNR\n",
    "tpr, tnr = ...\n",
    "print(f'True Positive Rate (TPR): {tpr:.6f}')\n",
    "print(f'True Negative Rate (TNR): {tnr:.6f}')\n",
    "\n",
    "# plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    # compute accuracy\n",
    "    acc = ...\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true.cpu(), y_pred.cpu(), labels=list(range(10)), normalize='pred')\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar(plt.imshow(cm))\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.title(f'Confusion Matrix (Closed-Set Accuracy: {acc:.2f})')\n",
    "    plt.xticks(ticks=range(10), labels=range(10))\n",
    "    plt.yticks(ticks=range(10), labels=range(10))\n",
    "    plt.show()\n",
    "\n",
    "# plot confusion matrix of known classes\n",
    "plot_confusion_matrix(..., ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
