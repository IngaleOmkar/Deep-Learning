{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xOnVXjNGgbz"
      },
      "source": [
        "### Group Members:\n",
        "\n",
        "- Mohammad Mahdi Hejazi, 24-748-998\n",
        "- Omkar Ingale, .....\n",
        "- Arjun, ....."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmubxigEE-Zb"
      },
      "source": [
        "# Assignment 1: Universal Function Approximator\n",
        "\n",
        "\n",
        "The goal of this exercise is to compare three different neural network architectures and analyze their capacity for function approximation:\n",
        "\n",
        "1. $N_1$: One-layer network (linear transformation only)\n",
        "2. $N_2$: One-layer network with non-linear activation function\n",
        "3. $N_3$: Two-layer network (hidden layer with non-linear activation function)\n",
        "\n",
        "They will be trained via gradient descent (with weight decay). To show the flexibility of the approach, three different functions will be approximated:\n",
        "1. $X_1: t = \\cos(3x)$ for $x\\in[-2,2]$\n",
        "2. $X_2: t = e^{-x^2}$ for $x\\in[-1,1]$\n",
        "3. $X_3: t = x^5 + 3x^4 - 6x^3 -12x^2 + 5x + 129$ for $x\\in[-4,2.5]$\n",
        "\n",
        "In the theoretical section, the networks will be designed, and the necessary derivatives will be computed by hand.\n",
        "\n",
        "In the coding section, we will:\n",
        "\n",
        "- implement the networks and their gradients,\n",
        "- generate target data for three different functions,\n",
        "- apply the training procedure to the data, and\n",
        "- plot the resulting approximated function together with the data samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n36PMmPANmJ7"
      },
      "source": [
        "## Section 1: Theoretical Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXFApo7obLKe"
      },
      "source": [
        "### Network Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH8oL0-MQ0IY"
      },
      "source": [
        "#### Task 1.1: Network Structure\n",
        "\n",
        "Given input $\\vec x = (1, x)^T$, define three neural networks ($N_1$, $N_2$, $N_3$) mathematically, to reach output $y$. Use $g()$ to represent the activation function.\n",
        "\n",
        "Explain how their structures differ and analyze their function approximation capabilities.\n",
        "\n",
        "---\n",
        "Note:\n",
        "\n",
        "For one-layer networks, define parameter $\\Theta=\\vec w \\in\\mathbb R^{D+1}$\n",
        "\n",
        "For two-layer network, define parameters $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$ that are split into $\\mathbf W^{(1)}\\in\\mathbb R^{K\\times {(D+1)}}$ for the first layer and $\\vec w^{(2)}\\in\\mathbb R^{K+1}$ for the second layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kujWaTnHTEnk"
      },
      "source": [
        "$N_1$: ...\n",
        "\n",
        "$N_2$: ...\n",
        "\n",
        "$N_3$: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSm46qR8SdQH"
      },
      "source": [
        "#### Task 1.2: Network Comparison\n",
        "\n",
        "Can the one-layer network approximate all three functions well? Why or why not?\n",
        "\n",
        "What advantages does the two-layer network have compared to a one-layer network?\n",
        "\n",
        "How can we determine the appropriate number of hidden neurons?\n",
        "When looking at the example plots in the OLAT, how many hidden neurons do we need in order to approximate the functions? Is there any difference between the three target functions?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_ZK0aMfafSP"
      },
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki1cAn6zSvj2"
      },
      "source": [
        "#### Task 1.3: Network Performance\n",
        "\n",
        "If the network struggles to approximate a function well, what are some possible reasons?\n",
        "\n",
        "How can we improve the network's performance?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP60Na-kbF_1"
      },
      "source": [
        "##### Reasons\n",
        "...\n",
        "\n",
        "##### Solutions\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPUvWGhybtv9"
      },
      "source": [
        "### Derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ibD4zrPOvE"
      },
      "source": [
        "#### Task 1.4: Activation Function\n",
        "\n",
        "Given the hyperbolic tangent ($\\tanh$) activation function as:\n",
        "\n",
        "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
        "\n",
        "Prove:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial x} \\tanh(x) = 1 - \\tanh^2(x)$$\n",
        "\n",
        "Hint: Apply the derivative rules as defined in the Lecture:\n",
        "* Quotient rule\n",
        "* Sum rule\n",
        "* Exponential rule\n",
        "\n",
        "Also, avoid factoring out parentheses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW6QrQKPUffH"
      },
      "source": [
        "$\\frac{\\partial}{\\partial x} \\tanh(x) =...$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC041GO7PzIP"
      },
      "source": [
        "#### Task 1.5: Weight Decay\n",
        "\n",
        "Consider a loss function with L2 regularization (weight decay):\n",
        "$$\n",
        "L'(\\theta) = L(\\theta) + \\frac{\\lambda}{2} \\|\\theta\\|^2\n",
        "$$\n",
        "\n",
        "Compute its derivative with respect to $\\theta$: $$\\frac{\\partial}{\\partial \\theta} L'(\\theta)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o4IjaZ4VimR"
      },
      "source": [
        "$\\frac{\\partial}{\\partial \\theta} L'(\\theta) = ...$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOiw6bNJQG5E"
      },
      "source": [
        "#### Task 1.6\n",
        "\n",
        "How large should an appropriate weight decay parameter $\\lambda$ as shown in Task 1.5 be? What would happen if $\\lambda$ is set too high or too low?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vErgVANhV9cu"
      },
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY8F0hc3Ttyn"
      },
      "source": [
        "## Section 2: Coding\n",
        "\n",
        "**<font color='red' size='5'>This section has to be submitted by 11:59 p.m. on Wednesday, March 12th, to be graded.</font>**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLvRiQ-6NRB8"
      },
      "source": [
        "### Network Implementation\n",
        "#### Task 2.1\n",
        "\n",
        "Recall that for one-layer networks, we define parameter $\\Theta=\\vec w \\in\\mathbb R^{D+1}$, and for a two-layer network, we define parameters $\\Theta=(\\mathbf W^{(1)}\\in\\mathbb R^{K\\times {(D+1)}},\\vec w^{(2)}\\in\\mathbb R^{K+1})$.\n",
        "\n",
        "- D: The dimension of the input. In this assignment, $D = 1$ since there is only one input.\n",
        "- K: The number of hidden neurons in the first layer of the two-layer network ($N_3$)\n",
        "\n",
        "Implement a function that returns the network output for a given input $\\vec x$, parameter(s) $\\Theta$, and model_type ($N_1$, $N_2$, or $N_3$). Remember that the input of the function $\\vec x = (1, x)^T$.\n",
        "\n",
        "---\n",
        "Note:\n",
        "\n",
        "1. Use the `numpy` to implement the $\\tanh$ function.\n",
        "2. Use `numpy.concatenate` or `numpy.insert` to prepend $h_0$.\n",
        "3. Make use of `numpy.dot` to compute matrix-vector and vector-vector products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4XOSElIwJ5BB"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "\n",
        "def network(x, Theta, model_type):\n",
        "    \"\"\"\n",
        "    Compute the output of a neural network model.\n",
        "\n",
        "    Args:\n",
        "        x: Input vector (1, x) including bias.\n",
        "        Theta: Tuple of network parameters (W1, w2).\n",
        "        model_type: Type of model (1, 2, or 3).\n",
        "\n",
        "    Returns:\n",
        "        y: Network output.\n",
        "        h: Hidden layer output, or None.\n",
        "    \"\"\"\n",
        "\n",
        "    W1, w2 = Theta # w2 is None if model_type is 1 or 2\n",
        "\n",
        "    if model_type == 1:\n",
        "        # One-layer network (Linear Model)\n",
        "        y = numpy.dot(W1, x)\n",
        "        return y, None # To make this consistent when model_type is 3\n",
        "\n",
        "    elif model_type == 2:\n",
        "        # One-layer network with tanh activation\n",
        "        y = numpy.tanh(numpy.dot(W1, x))\n",
        "        return y, None # To make this consistent when model_type is 3\n",
        "\n",
        "    elif model_type == 3:\n",
        "        # Two-layer network with tanh activation\n",
        "        a_ = numpy.dot(W1, x)\n",
        "        h_ = numpy.tanh(a_)\n",
        "        h = numpy.insert(h_, 0, 1)\n",
        "        y = numpy.dot(w2, h)\n",
        "        return y, h\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5XL_ohAE-Zh"
      },
      "source": [
        "#### Test 1: Sanity Check\n",
        "\n",
        "We select a specific number of hidden neurons and create the weights accordingly, using all zeros in the first layer and all ones in the second. The test case below ensures that the function from Task 1 actually returns $11$ for those weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ku3Sy5fzj8YH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N1 test passed.\n",
            "N2 test passed.\n",
            "N3 test passed.\n"
          ]
        }
      ],
      "source": [
        "# Define test parameters\n",
        "K = 20\n",
        "D = 1\n",
        "Theta_one_layer = [numpy.ones(D+1),None]\n",
        "Theta_two_layer = [numpy.zeros((K, D+1)), numpy.ones(K+1)]\n",
        "x = numpy.random.rand(D+1)\n",
        "\n",
        "# Sanity check for N1\n",
        "y1, _ = network(x, Theta_one_layer, 1)\n",
        "assert abs(numpy.sum(x) - y1) < 1e-6\n",
        "print(\"N1 test passed.\")\n",
        "\n",
        "# Sanity check for N2\n",
        "y2, _ = network(x, Theta_one_layer, 2)\n",
        "assert abs(numpy.tanh(numpy.sum(x)) - y2) < 1e-6\n",
        "print(\"N2 test passed.\")\n",
        "\n",
        "# Sanity check for N3\n",
        "y3, _ = network(x, Theta_two_layer, 3)\n",
        "assert abs(1.0 - y3) < 1e-6\n",
        "print(\"N3 test passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fW7_KzcE-Zi"
      },
      "source": [
        "### Gradient Implementation\n",
        "#### Task 2.2: Gradient Computation\n",
        "\n",
        "\n",
        "Implementation of a function that returns the gradient as defined for a given dataset $X=\\{(\\vec x^{[n]}, t^{[n]})\\}$, given weight(s) $\\Theta$, model_type ($N_1$, $N_2$, or $N_3$), and $\\lambda$ parameter for weight decay.\n",
        "\n",
        "---\n",
        "Note:\n",
        "\n",
        "We should make sure that both parts of the gradient are computed for $N_3$ (since $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$ here).\n",
        "\n",
        "This is a very slow implementation. We will see how to speed this up in the next lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3Vl_dYxcW-VD"
      },
      "outputs": [],
      "source": [
        "def compute_gradient(X, Theta, model_type, lambda_=1.):\n",
        "    \"\"\"\n",
        "    Compute the gradient of the loss function with respect to the weights for each model type.\n",
        "\n",
        "    Args:\n",
        "        X: Dataset containing input-target pairs (x, t).\n",
        "        Theta: Network parameters (W1, w2).\n",
        "        model_type: Type of model (1, 2, or 3).\n",
        "        lambda_: Weight decay parameter. Default is 1.0.\n",
        "\n",
        "    Returns:\n",
        "        Gradients with respect to W1 and w2. For model_type 1 and 2, w2 is None.\n",
        "    \"\"\"\n",
        "\n",
        "    # split parameters for easier handling\n",
        "    W1, w2 = Theta # w2 is None if model_type is 1 or 2\n",
        "\n",
        "    # define gradient with respect to both parameters\n",
        "    dW1 = numpy.zeros_like(W1)\n",
        "    dw2 = numpy.zeros_like(w2) if w2 is not None else None # dw2 is None if model_type is 1 or 2\n",
        "\n",
        "    # iterate over dataset\n",
        "    for x, t in X:\n",
        "        # compute the gradient\n",
        "        y, h = network(x, Theta, model_type)\n",
        "\n",
        "        # compute the error\n",
        "        e = y - t\n",
        "\n",
        "        # compute the gradient for the output layer\n",
        "        if model_type == 1:\n",
        "            dW1 += e * x[1]\n",
        "        elif model_type == 2:\n",
        "            dW1 += e * (1 - numpy.tanh(numpy.dot(W1, x[1]))**2) * x[1]\n",
        "        elif model_type == 3:\n",
        "            dW1 += e * (1 - h**2) * x[1]\n",
        "            dw2 += e * h\n",
        "\n",
        "    # Add penalty term/weight decay\n",
        "    dW1 += lambda_ * W1\n",
        "    if w2 is not None:\n",
        "        dw2 += lambda_ * w2\n",
        "\n",
        "    return dW1, dw2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqAMeoy5E-aG"
      },
      "source": [
        "#### Task 2.3: Gradient Descent\n",
        "\n",
        "The procedure of gradient descent is the repeated application of two steps.\n",
        "\n",
        "1. The gradient of loss $\\nabla_{\\Theta}\\mathcal J^{L_2}$ is computed based on the current value of the parameters $\\Theta$.\n",
        "2. The weights are updated by moving a small step in the direction of the negative gradient:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    \\Theta = \\Theta - \\eta \\nabla_{\\Theta}\\mathcal J\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "As a stopping criterion, we select the number of training epochs to be 10000.\n",
        "\n",
        "Implementation of a function that performs gradient descent for a given dataset $X$, given initial parameters $\\Theta$, a given learning rate $\\eta$, model_type ($N_1$, $N_2$, or $N_3$), and $\\lambda$ parameter for weight decay, and returns the optimized parameters $\\Theta^*$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hx6Jjs2e2CFX"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, Theta, eta, model_type, lambda_=1.):\n",
        "    epochs = 10000\n",
        "\n",
        "    # perform iterative gradient descent\n",
        "    for _ in range(epochs):\n",
        "        # compute the gradient\n",
        "        dW1, dw2 = compute_gradient(X, Theta, model_type, lambda_)\n",
        "        dW1 = -eta * dW1\n",
        "        dw2 = -eta * dw2 if dw2 is not None else None\n",
        "\n",
        "        # update the parameters\n",
        "        Theta[0] += dW1\n",
        "        if dw2 is not None:\n",
        "            Theta[1] += dw2\n",
        "\n",
        "    # return optimized parameters\n",
        "    return Theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gEeh7N8E-aH"
      },
      "source": [
        "### Datasets\n",
        "\n",
        "#### Task 2.4: Data Samples\n",
        "\n",
        "In total, we will test our gradient descent function with three different datasets. Particularly, we approximate\n",
        "\n",
        "1. $X_1: t = \\cos(3x)$ for $x\\in[-2,2]$\n",
        "2. $X_2: t = e^{-x^2}$ for $x\\in[-1,1]$\n",
        "3. $X_3: t = x^5 + 3x^4 - 6x^3 -12x^2 + 5x + 129$ for $x\\in[-4,2.5]$\n",
        "\n",
        "Generate dataset $X_1$,  for $N=60$ samples randomly drawn from range $x\\in[-2,2]$. Generate data $X_2$ for $N=50$ samples randomly drawn from range $x\\in[-1,1]$. Generate dataset $X_3$ for $N=200$ samples randomly drawn from range $x\\in[-4,2.5]$. Implement all three datasets as lists of tuples: $\\{(\\vec x^{[n]}, t^{[n]})\\mid 1\\leq n\\leq N\\}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1DdPBymdcNXx"
      },
      "outputs": [],
      "source": [
        "X1 = [((1, num), numpy.cos(3 * num)) for num in numpy.random.uniform(-2, 2, 60)]\n",
        "X2 = [((1, num), numpy.exp(-1 * num**2)) for num in numpy.random.uniform(-1, 1, 50)]\n",
        "X3 = [((1, num), num**5 + 3 * num**4 - 6 * num**3 - 12 * num**2 + 5 * num + 129) for num in numpy.random.uniform(-4, 2.5, 200)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tGZqaiUE-aH"
      },
      "source": [
        "#### Test 2: Sanity Check\n",
        "\n",
        "The test case below ensures that the elements of each generated dataset are tuples with two dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WrneyBJLE-aI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test passed!\n"
          ]
        }
      ],
      "source": [
        "assert all(\n",
        "    isinstance(x, (tuple,list)) and\n",
        "    len(x) == 2 and\n",
        "    isinstance(x[0], (tuple,list,numpy.ndarray)) and\n",
        "    len(x[0]) == 2 and\n",
        "    isinstance(x[1], float)\n",
        "    for X in (X1, X2, X3)\n",
        "    for x in X\n",
        ")\n",
        "\n",
        "print('Test passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0p9mC4YE-aI"
      },
      "source": [
        "### Function Approximation\n",
        "Finally, we want to make use of our gradient descent implementation to approximate our functions. In order to see our success, we want to plot the functions together with the data.\n",
        "\n",
        "#### Task 2.5: Define hidden Neurons\n",
        "How many hidden neurons will we need for $N_3$? Use the answers from Task 1.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OLichgq7cNXy"
      },
      "outputs": [],
      "source": [
        "# Define the number of neurons for each target function based on your discussion\n",
        "K1 = 10\n",
        "K2 = 2\n",
        "K3 = 80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fvx31eyE-aI"
      },
      "source": [
        "#### Task 2.6: Random Parameters\n",
        "\n",
        "For each of the networks, randomly initialize the parameters $\\Theta_1,\\Theta_2,\\Theta_3\\in[-1,1]$ for each of the datasets.\n",
        "\n",
        "For $N_3$, use the number of hidden neurons estimated in Task 1.2 and implemented in Task 2.5.\n",
        "\n",
        "---\n",
        "Note:\n",
        "\n",
        "  1. You can use `numpy.random.uniform` to initialize the weights.\n",
        "  2. Make sure that the weight matrices are instantiated in the correct dimensions.\n",
        "  3. Theta should always have two elements. The second element can be `None` for one-layer networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Aq768_chcNXy"
      },
      "outputs": [],
      "source": [
        "Theta_N1 = {\n",
        "    \"X1\": [numpy.random.uniform(-1, 1, (2,)), None],\n",
        "    \"X2\": [numpy.random.uniform(-1, 1, (2,)), None],\n",
        "    \"X3\": [numpy.random.uniform(-1, 1, (2, 80)), numpy.random.uniform(-1, 1, (81,))]\n",
        "}\n",
        "\n",
        "Theta_N2 = {\n",
        "    \"X1\": [numpy.random.uniform(-1, 1, (2,)), None],\n",
        "    \"X2\": [numpy.random.uniform(-1, 1, (2,)), None],\n",
        "    \"X3\": [numpy.random.uniform(-1, 1, (2, 80)), numpy.random.uniform(-1, 1, (81,))]\n",
        "}\n",
        "\n",
        "Theta_N3 = {\n",
        "    \"X1\": [numpy.random.uniform(-1, 1, (2,)), None],\n",
        "    \"X2\": [numpy.random.uniform(-1, 1, (2,)), None],\n",
        "    \"X3\": [numpy.random.uniform(-1, 1, (2, 80)), numpy.random.uniform(-1, 1, (81,))]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([-0.03262065, -0.03262065]), None]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gradient_descent(X1, Theta_N1['X1'], 0.01, 1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jCM2nyRE-aI"
      },
      "source": [
        "#### Task 2.7: Run Gradient Descent\n",
        "\n",
        "For each network, call gradient descent function from Task 2.3 using the datasets $X_1, X_2, X_3$, the according created parameters $\\Theta_1,\\Theta_2,\\Theta_3$. Store the resulting optimized weights $\\Theta_1^*, \\Theta_2^*, \\Theta_3^*$.\n",
        "\n",
        "Based on your chosen learning rates $\\eta$ and weight decay parameter $\\lambda$, you may need to optimize them for these functions. Do you see any differences? What are the best learning rates that you can find?\n",
        "\n",
        "---\n",
        "<span style=\"color:red\">WARNING: Depending on the implementation, this might run for several minutes!</span>\n",
        "\n",
        "---\n",
        "Note:\n",
        "\n",
        "1. Start with $\\eta=0.1$ and play around with the learning rate improve adaptation.\n",
        "2. $\\eta=0.1$ is too large for $X_3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4sjPUH_cNXz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/76/w363ly_90ln6pf3txlrc64bw0000gn/T/ipykernel_61752/785685704.py:32: RuntimeWarning: overflow encountered in add\n",
            "  dW1 += e * x[1]\n"
          ]
        }
      ],
      "source": [
        "# N1\n",
        "# Call gradient descent function using the datasets and initial weights that you created above\n",
        "# Choose appropriate learning rates for each function\n",
        "n1_x1_opt = gradient_descent(X1, Theta_N1[\"X1\"], eta = 0.1, model_type = 1, lambda_ = 0.01)\n",
        "n1_x2_opt = gradient_descent(X2, Theta_N1[\"X2\"], eta = 0.1, model_type = 1, lambda_ = 0.1)\n",
        "n1_x3_opt = gradient_descent(X3, Theta_N1[\"X3\"], eta = 0.1, model_type = 1, lambda_ = 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uio-gjhi3GBx"
      },
      "outputs": [],
      "source": [
        "# N2\n",
        "# Call gradient descent function using the datasets and initial weights that you created above\n",
        "# Choose appropriate learning rates for each function\n",
        "n2_x1_opt = gradient_descent(X1, Theta_N2[\"X1\"], eta = 0.1, model_type = 2, lambda_ = 0.1)\n",
        "n2_x2_opt = gradient_descent(X2, Theta_N2[\"X2\"], eta = 0.1, model_type = 2, lambda_ = 0.1)\n",
        "n2_x3_opt = gradient_descent(X3, Theta_N2[\"X3\"], eta = 0.1, model_type = 2, lambda_ = 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCPF-Q3C3GJk"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# N3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Call gradient descent function using the datasets and initial weights that you created above\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Choose appropriate learning rates for each function\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m n3_x1_opt \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTheta_N3\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m n3_x2_opt \u001b[38;5;241m=\u001b[39m gradient_descent(X2, Theta_N3[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX2\u001b[39m\u001b[38;5;124m\"\u001b[39m], eta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, lambda_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# n3_x3_opt = gradient_descent(X3, Theta_N3[\"X3\"], eta = 0.1, model_type = 3, lambda_ = 0.1)\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X, Theta, eta, model_type, lambda_)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# perform iterative gradient descent\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# compute the gradient\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     dW1, dw2 \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     dW1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39meta \u001b[38;5;241m*\u001b[39m dW1\n\u001b[1;32m      9\u001b[0m     dw2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39meta \u001b[38;5;241m*\u001b[39m dw2 \u001b[38;5;28;01mif\u001b[39;00m dw2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36mcompute_gradient\u001b[0;34m(X, Theta, model_type, lambda_)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# iterate over dataset\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, t \u001b[38;5;129;01min\u001b[39;00m X:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# compute the gradient\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     y, h \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# compute the error\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     e \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39m t\n",
            "Cell \u001b[0;32mIn[1], line 34\u001b[0m, in \u001b[0;36mnetwork\u001b[0;34m(x, Theta, model_type)\u001b[0m\n\u001b[1;32m     32\u001b[0m h_ \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mtanh(a_)\n\u001b[1;32m     33\u001b[0m h \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39minsert(h_, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, h\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
          ]
        }
      ],
      "source": [
        "# N3\n",
        "# Call gradient descent function using the datasets and initial weights that you created above\n",
        "# Choose appropriate learning rates for each function\n",
        "n3_x1_opt = gradient_descent(X1, Theta_N3[\"X1\"], eta = 0.1, model_type = 3, lambda_ = 0.1)\n",
        "n3_x2_opt = gradient_descent(X2, Theta_N3[\"X2\"], eta = 0.1, model_type = 3, lambda_ = 0.1)\n",
        "n3_x3_opt = gradient_descent(X3, Theta_N3[\"X3\"], eta = 0.1, model_type = 3, lambda_ = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8_C3TseE-aJ"
      },
      "source": [
        "### Data and Function Plotting\n",
        "\n",
        "### Task 2.8: Plotting Function\n",
        "\n",
        "Implement a plotting function that takes a given dataset $X$, given parameters $\\Theta$, model_type, and a defined range $R=[\\min,\\max]$. Each data sample $(x^{[n]},t^{[n]})$ of the dataset is plotted as an $''x''$. In order to plot the function that is approximated by the network, generate sufficient equally-spaced input values $x\\in R$, compute the network output $y$ for these inputs, and plot them with a line.\n",
        "\n",
        "---\n",
        "Note:\n",
        "\n",
        "  1. The dataset $X$ is defined as above, a list of tuples $(\\vec x, t)$.\n",
        "  2. Each input in the dataset is defined as $\\vec x = (1,x)^T$.\n",
        "  3. Equidistant points can be obtained via `numpy.arange`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6sII26VJcNXz"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "def plot(X, Theta, model_type, R):\n",
        "  # first, plot data samples\n",
        "  pyplot.plot([x[0] for x in X], [t for _, t in X], \"rx\", label=\"Data\")\n",
        "\n",
        "  # define equidistant points from min (R[0]) to max (R[1]) to evaluate the network\n",
        "  x = numpy.linspace(R[0], R[1], 100)\n",
        "  X_new = numpy.array([[1, x_] for x_ in x])\n",
        "  # compute the network outputs for these values\n",
        "  y = network(X_new, Theta, model_type)\n",
        "  # plot network approximation\n",
        "  pyplot.plot(x,y,\"k-\", label=\"network\")\n",
        "  pyplot.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXcO5e-sE-aJ"
      },
      "source": [
        "#### Task 2.9: Plot Three Functions\n",
        "\n",
        "For each of the datasets and their optimized parameters, call the plotting function from Task 2.8. Use range $R=[-3,3]$ for dataset $X_1$, range $R=[-2,2]$ for $X_2$, and range $R=[-5,4]$ for dataset $X_3$.\n",
        "\n",
        "Note that the first element of range $R$ should be the lowest $x$-location, and the second element of $R$ is the highest value for $x$.\n",
        "\n",
        "Repeat for three networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CY3YGkXgcNXz"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "shapes (2,) and (100,2) not aligned: 2 (dim 0) != 100 (dim 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# plot first function\u001b[39;00m\n\u001b[1;32m      4\u001b[0m pyplot\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m131\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn1_x1_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m plot(X2, n1_x2_opt, \u001b[38;5;241m1\u001b[39m, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# plot(X3, n1_x3_opt, 1, [-3, 3])\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# plot second function\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[18], line 10\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(X, Theta, model_type, R)\u001b[0m\n\u001b[1;32m      8\u001b[0m X_new \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m, x_] \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# compute the network outputs for these values\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# plot network approximation\u001b[39;00m\n\u001b[1;32m     12\u001b[0m pyplot\u001b[38;5;241m.\u001b[39mplot(x,y,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk-\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[1], line 21\u001b[0m, in \u001b[0;36mnetwork\u001b[0;34m(x, Theta, model_type)\u001b[0m\n\u001b[1;32m     17\u001b[0m W1, w2 \u001b[38;5;241m=\u001b[39m Theta \u001b[38;5;66;03m# w2 is None if model_type is 1 or 2\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# One-layer network (Linear Model)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y, \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# To make this consistent when model_type is 3\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# One-layer network with tanh activation\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (2,) and (100,2) not aligned: 2 (dim 0) != 100 (dim 0)"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "figure = pyplot.figure(figsize=(10,3))\n",
        "\n",
        "# plot first function\n",
        "pyplot.subplot(131)\n",
        "plot(X1, n1_x1_opt, 1, [-3, 3])\n",
        "plot(X2, n1_x2_opt, 1, [-3, 3])\n",
        "# plot(X3, n1_x3_opt, 1, [-3, 3])\n",
        "\n",
        "# plot second function\n",
        "pyplot.subplot(132)\n",
        "plot(X2, n2_x1_opt, 2, [-2, 2])\n",
        "plot(X2, n2_x2_opt, 2, [-2, 2])\n",
        "# plot(X2, n2_x3_opt, 2, [-2, 2])\n",
        "\n",
        "# plot third function\n",
        "pyplot.subplot(133)\n",
        "plot(X3, n3_x1_opt, 3, [-5, 4])\n",
        "plot(X3, n3_x2_opt, 3, [-5, 4])\n",
        "# plot(X3, n3_x3_opt, 3, [-5, 4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
